{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from pathlib import Path\n",
    "from espnet2.text.phoneme_tokenizer import PhonemeTokenizer\n",
    "from espnet2.text.token_id_converter import TokenIDConverter\n",
    "from espnet2.bin.tts_inference import Text2Speech\n",
    "\n",
    "from prosody.pinyin import *\n",
    "from prosody.en_to_zh import PinyinArpaSpeech, all_tones\n",
    "\n",
    "PWD = %pwd\n",
    "PWD = Path(PWD)\n",
    "LJSPEECH_DIR = (PWD / '../egs2/ljspeech/tts1/').resolve()\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:It seems weight norm is not applied in the pretrained model but the current model uses it. To keep the compatibility, we remove the norm from the current model. This may causes training error due to the parameter mismatch when finetuning. To avoid this issue, please change the following parameters in config to false:\n",
      " - discriminator_params.follow_official_norm\n",
      " - discriminator_params.scale_discriminator_params.use_weight_norm\n",
      " - discriminator_params.scale_discriminator_params.use_spectral_norm\n",
      " See also: https://github.com/espnet/espnet/pull/5240\n"
     ]
    }
   ],
   "source": [
    "os.chdir(LJSPEECH_DIR)\n",
    "pretrained_dir = LJSPEECH_DIR / \"exp/tts_train_jets_raw_phn_tacotron_g2p_en_no_space\"\n",
    "pretrained_model_file = pretrained_dir / \"train.total_count.ave_5best.pth\"\n",
    "pretrained_tts = Text2Speech.from_pretrained(\n",
    "    train_config=pretrained_dir / \"config_prosody.yaml\",\n",
    "    model_file=pretrained_model_file,\n",
    "    device=device\n",
    ")\n",
    "pretrained_model = pretrained_tts.model\n",
    "os.chdir(PWD)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "arpa_tokenizer = PhonemeTokenizer(g2p_type='g2p_en_no_space')\n",
    "id_converter = TokenIDConverter(pretrained_tts.train_args.token_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "save_dir = PWD / 'tts_train_jets_raw_phn_tacotron_g2p_en_no_space/mandarin'\n",
    "pas = PinyinArpaSpeech(token_id_converter=id_converter, tts_inference_fn=pretrained_model.tts.generator.inference)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'zhɨ1 zhɨ2 zhɨ3 zhɨ4 zha1 zha2 zha3 zha4 zhan1 zhan2 zhan3 zhan4 zhang1 zhang2 zhang3 zhang4 zhai1 zhai2 zhai3 zhai4 zhao1 zhao2 zhao3 zhao4 zhe1 zhe2 zhe3 zhe4 zhen1 zhen2 zhen3 zhen4 zheng1 zheng2 zheng3 zheng4 zhei1 zhei2 zhei3 zhei4 zhong1 zhong2 zhong3 zhong4 zhou1 zhou2 zhou3 zhou4 zhu1 zhu2 zhu3 zhu4 zhua1 zhua2 zhua3 zhua4 zhuo1 zhuo2 zhuo3 zhuo4 zhuai1 zhuai2 zhuai3 zhuai4 zhui1 zhui2 zhui3 zhui4 zhuan1 zhuan2 zhuan3 zhuan4 zhun1 zhun2 zhun3 zhun4 zhuang1 zhuang2 zhuang3 zhuang4'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_pinyins = INITIAL_TO_PINYINS['zh']\n",
    "initial_pinyin_tones = all_tones(initial_pinyins)\n",
    "' '.join(initial_pinyin_tones)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'duang1 duang2 duang3 duang4 guang1 guang2 guang3 guang4 kuang1 kuang2 kuang3 kuang4 huang1 huang2 huang3 huang4 zhuang1 zhuang2 zhuang3 zhuang4 chuang1 chuang2 chuang3 chuang4 shuang1 shuang2 shuang3 shuang4'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rime_pinyins = RIME_TO_PINYINS['uang']\n",
    "rime_pinyin_tones = all_tones(rime_pinyins)\n",
    "' '.join(rime_pinyin_tones)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -1         0    1    2    3    4    5    6    7         8        9    10    11    12         13         14    15    16    17         18          19     20\n",
      "    arpas        T   SH    W   AH1  NG    T   SH    W        AH1      NG    T     SH    W         AH1         NG    T     SH    W          AH1         NG     .\n",
      "   d_factor     1.0  0.0  1.0  1.0  1.0  1.0  0.0  1.0       1.0      1.0  1.0   0.0   1.0        1.0        1.0   1.0   0.0   1.0         1.0         1.0   1.0\n",
      "d_split_factor   1    1    1    2    1    1    1    1         3        1    1     1     1          3          1     1     1     1           3           1     1\n",
      "    tones        1    1    1    1    1    2    2    2         2        2    3     3     3          3          3     4     4     4           4           4     0\n",
      " pitch_values    2    2    2    2    2   -1   -1   -0.4  0.2|0.8|1.4   2    -1    -1   -1.4  -1.8|-1.8|-1.4   -1   1.5   1.52  0.94  0.36|-0.22|-0.8  -1.38   -1\n",
      "Duration pred: tensor([ 8.5702,  9.4827,  7.6087,  5.1322,  5.1890,  6.8939,  8.4443,  7.2471,\n",
      "         5.3291,  5.3526,  7.3366,  9.1340,  7.3454,  5.5271,  5.3819,  6.3203,\n",
      "        10.4868, 10.9771,  9.9967,  9.5012,  8.2628], device='cuda:0')\n",
      "Pitch pred: tensor([ 0.8327,  1.0839,  1.4065,  1.6852,  1.7189,  1.4697,  1.2742,  1.2364,\n",
      "         0.9833,  0.8405,  0.7066,  0.6112,  0.3074, -0.0390,  0.0097,  0.0438,\n",
      "         0.2865,  0.1892, -0.7898, -0.9561, -0.9672], device='cuda:0')\n",
      "Energy pred: tensor([-0.3941, -0.1184,  0.3849,  0.7251, -0.2168, -0.4562, -0.4738,  0.1525,\n",
      "         0.5042, -0.3498, -0.4504, -0.5359,  0.0963,  0.3826, -0.4745, -0.4481,\n",
      "        -0.5776,  0.2408, -0.0757, -0.8630, -1.0681], device='cuda:0')\n",
      "Duration (new): tensor([ 9,  0,  8,  4,  5,  5,  7,  0,  7,  3,  3,  3,  5,  7,  0,  7,  3,  3,\n",
      "         3,  5,  6,  0, 11,  3,  4,  3, 10,  8], device='cuda:0')\n",
      "Pitch (new): tensor([ 2.0000,  2.0000,  2.0000,  2.0000,  2.0000,  2.0000, -1.0000, -1.0000,\n",
      "        -0.4000,  0.2000,  0.8000,  1.4000,  2.0000, -1.0000, -1.0000, -1.4000,\n",
      "        -1.8000, -1.8000, -1.4000, -1.0000,  1.5000,  1.5192,  0.9385,  0.3577,\n",
      "        -0.2231, -0.8038, -1.3846, -1.0000], device='cuda:0')\n",
      "Energy (new): tensor([-0.3941, -0.1184,  0.3849,  0.7251,  0.7251, -0.2168, -0.4562, -0.4738,\n",
      "         0.1525,  0.5042,  0.5042,  0.5042, -0.3498, -0.4504, -0.5359,  0.0963,\n",
      "         0.3826,  0.3826,  0.3826, -0.4745, -0.4481, -0.5776,  0.2408, -0.0757,\n",
      "        -0.0757, -0.0757, -0.8630, -1.0681], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "chinese = all_tones('zhuang') + ['.']\n",
    "# chinese = rime_pinyin_tones[0:20] + ['.']\n",
    "# chinese = rime_pinyin_tones[20:40] + ['.']\n",
    "# chinese = rime_pinyin_tones[40:60] + ['.']\n",
    "# chinese = rime_pinyin_tones[60:80] + ['.']\n",
    "# chinese = '我提到的这个问题并不难处理.'\n",
    "# chinese = '这个议会代表着欧洲民众.'\n",
    "# chinese = '我相信你在那裡涉及到了某個要點.'\n",
    "# chinese = '各項報導反映出這種主流態度.'\n",
    "# chinese = initial_pinyin_tones[0:20] + ['.']\n",
    "# chinese = initial_pinyin_tones[20:40] + ['.']\n",
    "# chinese = initial_pinyin_tones[40:60] + ['.']\n",
    "# chinese = '他晕倒了.'\n",
    "\n",
    "pas_update_dict = {\n",
    "    'pinyin_to_arpa_durations': {},\n",
    "    'tone_duration_split': {},\n",
    "    'tone_contours': {},\n",
    "    'nucleus_tone_only': False,\n",
    "    'max_pitch_change': 2.5,\n",
    "}\n",
    "infer_overrides = {\n",
    "    # 'd_split_factor': None,\n",
    "    # 'd_factor': None,\n",
    "    # 'p_factor': None,\n",
    "    # 'e_factor': None,\n",
    "    # 'd_mod_fns': None,\n",
    "    # 'p_mod_fns': None,\n",
    "    # 'e_mod_fns': None,\n",
    "}\n",
    "inputs = pas.gen_audio(\n",
    "    chinese,\n",
    "    save_dir,\n",
    "    inputs=None,\n",
    "    pac_update_dict=pas_update_dict,\n",
    "    infer_overrides=infer_overrides,\n",
    "    overall_d_factor=1.0,\n",
    "    vowel_duration=(9.0, 15.0),\n",
    "    arpa_in_filename=len(chinese)<10,\n",
    "    device=device,\n",
    ")\n",
    "arpas, tones, d_factor, d_split_factor, pitch_values, p_mod_fns = inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
